{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4,
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Bag of words\n",
                "\n",
                "Bag of words (1-gram) counts the words and keep the numbers and serve as the features.\n",
                "\n",
                "2 steps:\n",
                "\n",
                "Tokenizing: Segement the corpus into \"words\"\n",
                "\n",
                "Counting: Count the appearance frequecy of each word.\n",
                "\n",
                "CountVectorizer from sklearn combine the tokenizing and counting. You can take a look at its definition from the following link. \n",
                " https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "\u003cstyle\u003e#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}\u003c/style\u003e\u003cdiv id=\"sk-container-id-1\" class=\"sk-top-container\"\u003e\u003cdiv class=\"sk-text-repr-fallback\"\u003e\u003cpre\u003eCountVectorizer()\u003c/pre\u003e\u003cb\u003eIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. \u003cbr /\u003eOn GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\u003c/b\u003e\u003c/div\u003e\u003cdiv class=\"sk-container\" hidden\u003e\u003cdiv class=\"sk-item\"\u003e\u003cdiv class=\"sk-estimator sk-toggleable\"\u003e\u003cinput class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked\u003e\u003clabel for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\"\u003eCountVectorizer\u003c/label\u003e\u003cdiv class=\"sk-toggleable__content\"\u003e\u003cpre\u003eCountVectorizer()\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e\u003c/div\u003e",
                        "text/plain": "CountVectorizer()"
                    },
                    "execution_count": 1,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from sklearn.feature_extraction.text import CountVectorizer\n",
                "\n",
                "vectorizer = CountVectorizer()\n",
                "vectorizer"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's create a corpus or collection of documents with four documents"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "array(['and', 'document', 'entry', 'first', 'is', 'one', 'or', 'second',\n       'the', 'third', 'this'], dtype=object)"
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "corpus = ['This is the first document or first entry.',\n",
                "          'This is the second second document.',\n",
                "          'And the third one.',\n",
                "          'Is this the first document?'\n",
                "         ]\n",
                "\n",
                "# Learn the vocabulary dictionary and return term-document matrix\n",
                "X_txt = vectorizer.fit_transform(corpus) \n",
                "\n",
                "# check the feature names after transformation. It will return individual word.\n",
                "vectorizer.get_feature_names_out() "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "[[0 1 1 2 1 0 1 0 1 0 1]\n [0 1 0 0 1 0 0 2 1 0 1]\n [1 0 0 0 0 1 0 0 1 1 0]\n [0 1 0 1 1 0 0 0 1 0 1]]\n"
                }
            ],
            "source": [
                "# X is the BoW feature of X\n",
                "print(X_txt.toarray())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "{'this': 10, 'is': 4, 'the': 8, 'first': 3, 'document': 1, 'or': 6, 'entry': 2, 'second': 7, 'and': 0, 'third': 9, 'one': 5}\n"
                }
            ],
            "source": [
                "# column index mapping to each word\n",
                "print(vectorizer.vocabulary_)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# unseen words are ignored for test data\n",
                "vectorizer.transform(['Something completely new.']).toarray()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### N-gram (N \u003e 1)\n",
                "\n",
                "Bag of Words features can't caputure local information such as order of words.\n",
                "\n",
                "\u003eE.g. \"believe or not\" has the same features as \"not or believe\".\n",
                "\n",
                "Bi-gram preserve more local information, which regrads 2 contagious words as one word in the vocabulary. In the example, \"believe or\", \"or not\", \"not or\" and \"or believe\" are counted. \n",
                "\n",
                "The feature is shown in the code below."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "['believe or', 'or not', 'not a', 'a b', 'b c', 'c d', 'd e']\n"
                }
            ],
            "source": [
                "bigram_vectorizer = CountVectorizer(ngram_range=(2, 2),       # range of the number of words inside a vocabulary\n",
                "                                    token_pattern=r'\\b\\w+\\b', # define the format of 'word': any char or number between 2 symbols except '_'\n",
                "                                    min_df = 1)               # ignore the words that appears less than `min_df` times\n",
                "\n",
                "analyze = bigram_vectorizer.build_analyzer() # Return a callable that handles preprocessing and tokenization\n",
                "print(analyze('believe or not a b c d e'))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Extract bi-gram features for the corpus"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "[[0 1 1 1 1 0 1 0 0 1 0 0 0 1 0]\n [0 0 0 0 1 0 0 1 1 0 1 0 0 1 0]\n [1 0 0 0 0 0 0 0 0 0 0 1 1 0 0]\n [0 0 1 0 0 1 0 0 0 1 0 0 0 0 1]]\n"
                }
            ],
            "source": [
                "X_txt_2 = bigram_vectorizer.fit_transform(corpus).toarray()\n",
                "print(X_txt_2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "array(['and the', 'document or', 'first document', 'first entry',\n       'is the', 'is this', 'or first', 'second document',\n       'second second', 'the first', 'the second', 'the third',\n       'third one', 'this is', 'this the'], dtype=object)"
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "bigram_vectorizer.get_feature_names_out()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### tf-idf\n",
                "Some words has very high frequency(e.g. “the”, “a”, ”which”), and therefore, carrying not much meaningful information about the actual contents of the document.\n",
                "\n",
                "We need to compensate them to prevent the high-frequency shadowing of other words. tf-idf (term frequency-inverse document frequency) is used to reflect the importance of a word to a document in a collection or corpus. There are two terms in the tf-idf weight: term frequence (tf) and inverse document frequency (idf). \u003cbr\u003e\n",
                "\n",
                "TF measures how frequently a term occurs in a document. It is often divided by the document length or the total number of terms in the document as a way of normalization. \u003cbr\u003e\n",
                "\n",
                "$$tf(t,d)=\\frac{\\text{Number of times term t appears in a document}}{\\text{Total number of terms in the document}}$$\n",
                "\n",
                "\n",
                "Idf measures how important a term is. In tf, all terms have equal importance. However, certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times, but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following.\n",
                "\n",
                "$$idf(t) = log(\\frac{1 + n_d}{1 + df(t)}) + 1$$\n",
                "\n",
                "- $n_d$ is the number of document.\n",
                "- $df(t)$ is the number of documents containing $t$. \u003cbr\u003e\n",
                "\n",
                "Finally tf-idf is calculated as follows.\n",
                "\n",
                "$$tf\\text{-}idf(t, d) = tf(t, d) \\times idf(t)$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "import numpy as np"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "['This is the first document or first entry.', 'This is the second second document.', 'And the third one.', 'Is this the first document?']\n"
                }
            ],
            "source": [
                "print(corpus)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "[[0.         0.23981982 0.444427   0.63066849 0.23981982 0.\n  0.444427   0.         0.18624148 0.         0.23981982]\n [0.         0.24014568 0.         0.         0.24014568 0.\n  0.         0.89006176 0.18649454 0.         0.24014568]\n [0.56115953 0.         0.         0.         0.         0.56115953\n  0.         0.         0.23515939 0.56115953 0.        ]\n [0.         0.43306685 0.         0.56943086 0.43306685 0.\n  0.         0.         0.33631504 0.         0.43306685]]\n"
                }
            ],
            "source": [
                "# 'norm = l2' means we want to normalize each row (document) to have a unit l2 norm\n",
                "# 'smooth_idf = False'. If True, 1 is added to the numerator and denominator of the idf to prevent zero divisions\n",
                "vectorizer = TfidfVectorizer(norm='l2', smooth_idf=False)\n",
                "\n",
                "X_txt_3 = vectorizer.fit_transform(corpus)\n",
                "print(X_txt_3.toarray())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "array([1., 1., 1., 1.])"
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# see the l2 norm is 1 for each row\n",
                "np.square(X_txt_3.toarray()).sum(axis=1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                ""
            ]
        }
    ]
}
